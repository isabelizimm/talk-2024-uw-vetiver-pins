---
title: "MLOps with vetiver"
subtitle: "January 2024"
author: "Isabel Zimmerman, Posit PBC"
format:
  revealjs: 
    slide-number: true
    preview-links: auto
    background-image: https://i.pinimg.com/originals/5f/84/f3/5f84f35a4861d27b91ef32837d50bf7f.jpg
    footer: http://www.isabelizimm.me/talk-2024-vetiver-pins/
---

## Hello!

. . .

:::: {.columns}

::: {.column width="50%"}

<img src="https://github.com/isabelizimm.png" style="border-radius: 50%;" width="300px"/>

:::

::: {.column width="50%"}

[{{< fa brands github >}} \@isabelizimm](https://github.com/isabelizimm)

[{{< fa brands mastodon >}} \@isabelizimm@fosstodon.org](https://fosstodon.org/@isabelizimm)

[{{< fa brands linkedin >}} isabel-zimmerman](https://www.linkedin.com/in/isabel-zimmerman/)

[{{< fa link >}} isabelizimm.github.io](https://www.isabelizimm.me)

:::

::::


## Who are you?

- Data scientist üë©‚Äçüíª

- Statistician üåü

- Data analyst üìà

- Software engineer üõ†Ô∏è

::: notes
or other data practitioner
:::

# MLOps is...

. . . 

_Machine Learning Operations_


::: {.notes}
the name of this talk is MLOps with vetiver, but you might not know what MLOps is, or you might have heard of MLOps but it came with a chart of the MLOps landscape that looks sort of like...
:::

# MLOps is...

![](https://ljvmiranda921.github.io/assets/png/mlops-shop/mlops-landscape.png)

::: notes
this

- new world
- still learning what MLOps is and how to define it

:::

# MLOps is...

. . .

a set of <u>practices</u> to *deploy* and *maintain* machine learning models in production **reliably** and **efficiently**

::: notes
these practices can help you create better models, not necessarily in the statistical sense

- model management
- model collaboration
- model relevancy

:::

# Who needs MLOps?

::: notes
- Where are our models stored?
- How do I share my model with the rest of my organization?
- How can I use a model without importing it and reproducing its environment?
- What is our process or cadence to retrain deployed models with new data?
- How do I integrate my model into a workflow orchestration system?
- How can I monitor model performance over time?
:::


# Who needs MLOps? 

::: r-fit-text
if you develop models...

you can operationalize them
:::

::: notes
but Posit believes if you are someone who develops models, you can and often should be the one to operationalize it, and you already have the a lot of the skills you need to do this effectively

the "real world" value of models often times comes from integrating

my advice for you, is to bring your models outside!!
:::

# {background-iframe="logo-fall/index.html"}

![](https://github.com/isabelizimm/pydata-nyc2022/blob/main/images/vetiverhex.png?raw=true){fig-align="center"}

::: notes
the tool the mlops team at posit has created is called vetiver, and it is available as an R and Python package
:::


##

![](https://github.com/isabelizimm/pydata-nyc2022/blob/main/images/ml_ops_cycle.png?raw=true)


# Version with vetiver

::: {.notes}
we version lots of different things, and mostly badly
:::

## Version with vetiver

`model`

. . .

`model_final`

. . .

`model_final_final`

. . . 

`model_final_final_actually`

. . . 

`model_final_final_actually (1)`

::: {.notes}
versioning your model is the foundation for success in machine learning deployments...

we can already see here this is not going to scale for ONE MODEL
lacks context

it would be nice if my models:
- lived in a central location
- were discoverable by my team
- loaded right into memory
:::

## Version with vetiver

:::: {.columns}

::: {.column width="50%"}
_in r_
```r
library(vetiver)
library(pins)

model_board <- board_temp()
```
:::

::: {.column width="50%"}
_in python_
```python
import vetiver
import pins

model_board = board_temp(
    allow_pickle_read = True
    )
```
:::

::::

::: {.notes}
vetiver has a secret best friend pins, which is also available as a package in r and python.

board holds, organizes, and created metadata for almost any objects, but it has some special features when you use a vetiver model
:::

## Version with vetiver

:::: {.columns}

::: {.column width="50%"}
_in r_
```r
library(vetiver)
library(pins)

model_board <- board_connect()

v <- vetiver_model(
    model, 
    "user.name/model_name"
    )
```
:::

::: {.column width="50%"}
_in python_
```python
import pins
from dotenv import load_dotenv
from vetiver import VetiverModel

load_dotenv()

model_board = pins.board_connect(
    allow_pickle_read = True)

v = VetiverModel(
    model, 
    "user.name/model_name", 
    training_data
    )
```
:::

::::

## Version with vetiver

:::: {.columns}

::: {.column width="50%"}
_in r_
```r
library(vetiver)
library(pins)

model_board <- board_connect()

v <- vetiver_model(
    model, 
    "user.name/model_name"
    )

model_board %>% 
    vetiver_pin_write(v)
```
:::

::: {.column width="50%"}
_in python_
```python
import pins
from dotenv import load_dotenv
from vetiver import VetiverModel

load_dotenv()

model_board = pins.board_connect(
    allow_pickle_read = True
    )

v = VetiverModel(
    model, 
    "user.name/model_name", 
    prototype_data = training_data
    )

vetiver_pin_write(model_board, v)
```
:::

::::

::: {.notes}
then, you write your pin to your board and pins will automatically version it for you. you can also read a specific version of the pin at a later date, and your model will be loaded right into memory!

we can see that pins helps with scale, but what about context?
:::

## Version with vetiver

:::: {.columns}

::: {.column width="50%"}

```r
model_board %>% 
    pin_meta("user.name/model_name")
```
:::

::: {.column width="50%"}
```python
model_board.pin_meta(
    "user.name/model_name"
    )
```
:::

::::


::: {.notes}
versioning 
:::

# Deploy your model

![](https://github.com/isabelizimm/pydata-nyc2022/blob/main/images/ml_ops_cycle.png?raw=true)

## Deploy your model

![](https://github.com/isabelizimm/pydata-nyc2022/blob/main/images/deploy-cloud.jpg?raw=true)

::: {.notes}
creating model as a REST API endpoint

useful bc model can be used in memory just like you loaded it! without having to load it

also useful since API endpoints are testable, 
:::

## Deploy your model

![](https://github.com/isabelizimm/pydata-nyc2022/blob/main/images/deploy-not-here.jpg?raw=true)

## Deploy your model

:::: {.columns}

::: {.column width="50%"}
_in r_
```r
library(plumber)

pr() %>%
  vetiver_api(v)
```
:::

::: {.column width="50%"}
_in python_
```python
api = VetiverAPI(v)
api.run()
```
:::

::::


## Deploy your model


:::: {.columns}

::: {.column width="50%"}
_in r_
```r
vetiver_deploy_rsconnect(
    model_board, 
    "user.name/model_name"
    )
```
:::

::: {.column width="50%"}
_in python_
```python
vetiver.deploy_rsconnect(
    connect_server, 
    model_board, 
    "user.name/model_name"
    )
```
:::

::::

# Monitor your model

![](https://github.com/isabelizimm/pydata-nyc2022/blob/main/images/ml_ops_cycle.png?raw=true)

## Monitor your model

![](https://github.com/isabelizimm/pydata-nyc2022/blob/main/images/decay.jpeg?raw=true)

::: {.notes}
model is deployed a data scientist's work is not done!

now, monitoring means somthing unique in MLOps-- not necessarily looking at CPU usage, runtime, etc, 

looking at statistical properties of input data or predictions
:::

## Monitor your model

```{.python code-line-numbers="1,10,16"}
metrics = vetiver.compute_metrics(
    new_data, 
    "date", 
    timedelta(weeks = 1), 
    [mean_absolute_error, r2_score], 
    "like_count", 
    "y_pred"
    )

vetiver.pin_metrics(
    model_board, 
    metrics, 
    "metrics_pin_name"
    )
    
vetiver.plot_metrics(metrics)
```

## Monitor your model

![](https://github.com/isabelizimm/pydata-nyc2022/blob/main/images/silent_error.jpeg?raw=true)

::: {.notes}
it is SO IMPORTANT TO TRACK your model's performance metrics start decaying.

software engineering--when things went wrong, ERROR

models fail silently! and they can still run with no error, even if your accuracy is zero percent--

if you are not monitoring your model in some way, you are oblivious to decay.

dashboards in R and Python
:::

## Make it easy to do the right thing

- robust and human-friendly checking of new data
- track and document software dependencies of models
- [model cards](https://vetiver.rstudio.com/learn-more/model-card.html) for transparent, responsible reporting

::: notes
Model Cards provide a framework for transparent, responsible reporting. 
 Use the vetiver `.qmd` Quarto template as a place to start, 
 with vetiver.model_card()
:::

## Demo

## Using vetiver

- Allows those new to MLOps to get started quickly
- Supports scaling safely as an org matures


::: {.notes}
few simple tools that you are able to compose together to make complex objects

since vetiverapi is built on fastapi, can build out to be quite complex
also has methods to add other POST endpoints

also is composable with other tools to build out a custom framework that works well for your team
---
one thing vetiver has worked really hard on is to lower the barrier to entry on deploying models, making this feel like a natural extension of your current data science workflow

you are still able to use the tools you want

also with leveraging pins, it makes it easy to move data between R and Python at places where this is possible
:::

## Using vetiver

- Version
- Deploy
- Monitor

your R and Python models!

## Resources

- Documentation at https://vetiver.rstudio.com/

- Isabel Zimmerman‚Äôs talk from rstudio::conf() 2022 on [Demystifying MLOps](https://www.rstudio.com/conference/2022/talks/demystifying-mlops/)

- Webinar by Julia and Isabel for [Posit Enterprise Meetup](https://juliasilge.github.io/mlops-rstudio-meetup/)

- Julia‚Äôs screencast on [deploying a model with Docker](https://www.youtube.com/watch?v=5s7fI4cl2C8)

- End-to-end demos from Posit Solution Engineering in [R](https://github.com/sol-eng/bike_predict) and [Python](https://github.com/sol-eng/bike_predict_python)